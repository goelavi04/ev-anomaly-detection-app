{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdfc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72854ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\ev-anamoly-detection\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74fff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "Notebook directory (where this code is running): c:\\ev-anamoly-detection\\ev-anomaly-detection\\notebooks\n",
      "Main project directory (ev-anomaly-detection): c:\\ev-anamoly-detection\\ev-anomaly-detection\n",
      "Ensured 'models' directory exists at: c:\\ev-anamoly-detection\\ev-anomaly-detection\\models\n",
      "âœ… Successfully loaded data from: c:\\ev-anamoly-detection\\ev-anomaly-detection\\data\\ev_charging_data.csv\n",
      "\n",
      "--- [1/2] Training DoS Model ---\n",
      "DoS model training complete.\n",
      "âœ… DoS model successfully saved to: c:\\ev-anamoly-detection\\ev-anomaly-detection\\models\\dos_model.pkl\n",
      "âœ… DoS scaler successfully saved to: c:\\ev-anamoly-detection\\ev-anomaly-detection\\models\\dos_scaler.pkl\n",
      "\n",
      "--- [2/2] Training Fraud Model ---\n",
      "Fraud model training complete.\n",
      "âœ… Fraud model successfully saved to: c:\\ev-anamoly-detection\\ev-anomaly-detection\\models\\fraud_model.pkl\n",
      "âœ… Fraud scaler successfully saved to: c:\\ev-anamoly-detection\\ev-anomaly-detection\\models\\fraud_scaler.pkl\n",
      "\n",
      "--- Model Training Script Finished. You can now start your backend server. ---\n"
     ]
    }
   ],
   "source": [
    "# === ALL-IN-ONE MODEL TRAINING SCRIPT ===\n",
    "# This single cell imports all libraries, defines paths, loads data,\n",
    "# trains both models, and saves them to the correct folder.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"--- Starting Model Training ---\")\n",
    "\n",
    "# --- 1. SETUP AND PATHS ---\n",
    "try:\n",
    "    # This robustly finds the notebook's directory\n",
    "    notebook_dir = Path(os.path.abspath(''))\n",
    "    print(f\"Notebook directory (where this code is running): {notebook_dir}\")\n",
    "\n",
    "    # Go up one level to the main project directory\n",
    "    project_dir = notebook_dir.parent\n",
    "    print(f\"Main project directory (ev-anomaly-detection): {project_dir}\")\n",
    "\n",
    "    # Define the correct paths for your data and models folders\n",
    "    data_path = project_dir / 'data' / 'ev_charging_data.csv'\n",
    "    models_dir = project_dir / 'models'\n",
    "\n",
    "    # Create the 'models' directory if it doesn't exist, just in case\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print(f\"Ensured 'models' directory exists at: {models_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ðŸš¨ ERROR setting up paths: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "try:\n",
    "    df = pd.read_csv(data_path, parse_dates=['start_time', 'end_time'])\n",
    "    print(f\"âœ… Successfully loaded data from: {data_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ðŸš¨ ERROR: Data file not found at {data_path}. Please run `generate_data.py` first.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ðŸš¨ ERROR loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- 3. TRAIN & SAVE DOS MODEL ---\n",
    "print(\"\\n--- [1/2] Training DoS Model ---\")\n",
    "try:\n",
    "    # Select features and scale\n",
    "    dos_features = ['cpu_usage_percent', 'packets_per_sec']\n",
    "    X_dos = df[dos_features]\n",
    "    scaler_dos = StandardScaler()\n",
    "    X_dos_scaled = scaler_dos.fit_transform(X_dos)\n",
    "\n",
    "    # Train the model (using 0.16 for 16% contamination from our data script)\n",
    "    dos_model = IsolationForest(n_estimators=100, contamination=0.16, random_state=42)\n",
    "    dos_model.fit(X_dos_scaled)\n",
    "    print(\"DoS model training complete.\")\n",
    "\n",
    "    # Save the model and scaler\n",
    "    dos_model_path = models_dir / 'dos_model.pkl'\n",
    "    dos_scaler_path = models_dir / 'dos_scaler.pkl'\n",
    "    joblib.dump(dos_model, dos_model_path)\n",
    "    joblib.dump(scaler_dos, dos_scaler_path)\n",
    "    \n",
    "    print(f\"âœ… DoS model successfully saved to: {dos_model_path}\")\n",
    "    print(f\"âœ… DoS scaler successfully saved to: {dos_scaler_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ðŸš¨ ERROR during DoS model training/saving: {e}\")\n",
    "\n",
    "\n",
    "# --- 4. TRAIN & SAVE FRAUD MODEL ---\n",
    "print(\"\\n--- [2/2] Training Fraud Model ---\")\n",
    "try:\n",
    "    # Select features and scale\n",
    "    fraud_features = ['energy_kWh', 'amount_INR']\n",
    "    X_fraud = df[fraud_features]\n",
    "    scaler_fraud = StandardScaler()\n",
    "    X_fraud_scaled = scaler_fraud.fit_transform(X_fraud)\n",
    "\n",
    "    # Train the model\n",
    "    fraud_model = IsolationForest(n_estimators=100, contamination=0.16, random_state=42)\n",
    "    fraud_model.fit(X_fraud_scaled)\n",
    "    print(\"Fraud model training complete.\")\n",
    "\n",
    "    # Save the model and scaler\n",
    "    fraud_model_path = models_dir / 'fraud_model.pkl'\n",
    "    fraud_scaler_path = models_dir / 'fraud_scaler.pkl'\n",
    "    joblib.dump(fraud_model, fraud_model_path)\n",
    "    joblib.dump(scaler_fraud, fraud_scaler_path)\n",
    "    \n",
    "    print(f\"âœ… Fraud model successfully saved to: {fraud_model_path}\")\n",
    "    print(f\"âœ… Fraud scaler successfully saved to: {fraud_scaler_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ðŸš¨ ERROR during Fraud model training/saving: {e}\")\n",
    "\n",
    "print(\"\\n--- Model Training Script Finished. You can now start your backend server. ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5cf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
